{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create a Pandas DataFrame from the dataset\n",
    "data = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "\n",
    "# Add the target variable to the DataFrame\n",
    "data['target'] = housing.target\n",
    "\n",
    "# 1. Normalize each feature using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[data.columns[:-1]] = scaler.fit_transform(data[data.columns[:-1]])\n",
    "\n",
    "# 2. Identify and drop extreme values (outliers)\n",
    "# You can define a threshold to determine what is considered an extreme value.\n",
    "# For example, you can use a z-score to detect outliers beyond a certain threshold.\n",
    "\n",
    "threshold = 3  # Adjust this threshold as needed\n",
    "\n",
    "# Calculate z-scores for each feature\n",
    "z_scores = (data[data.columns[:-1]] - data[data.columns[:-1]].mean()) / data[data.columns[:-1]].std()\n",
    "\n",
    "# Create a boolean mask for extreme values\n",
    "outlier_mask = (z_scores.abs() < threshold).all(axis=1)\n",
    "\n",
    "# Filter the DataFrame to remove rows with extreme values\n",
    "data = data[outlier_mask]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Now, data contains the normalized features, and extreme values have been removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from xDL.models.NAM import NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [03:01, 36.26s/it]\n",
      "4it [00:08,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "nam = NAM(\n",
    "    \"target ~  -1 + PolynomialSplineNet(Longitude; degree=8) + CubicSplineNet(AveRooms; n_knots=8)+ CubicSplineNet(MedInc; n_knots=8) + CubicSplineNet(Latitude; n_knots=8) \", \n",
    "    data=data, \n",
    "    feature_dropout=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=({'Longitude': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'AveRooms': TensorSpec(shape=(None, 8), dtype=tf.float64, name=None), 'MedInc': TensorSpec(shape=(None, 8), dtype=tf.float64, name=None), 'Latitude': TensorSpec(shape=(None, 8), dtype=tf.float64, name=None)}, TensorSpec(shape=(None,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam.training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Network architecture --------------\n",
      "Longitude -> PolynomialSplineNet(feature=Longitude, n_params=648) -> output dimension=1\n",
      "AveRooms -> CubicSplineNet(feature=AveRooms, n_params=648) -> output dimension=1\n",
      "MedInc -> CubicSplineNet(feature=MedInc, n_params=648) -> output dimension=1\n",
      "Latitude -> CubicSplineNet(feature=Latitude, n_params=648) -> output dimension=1\n",
      "16/16 [==============================] - 3s 24ms/step - loss: 26.6401 - output_loss: 26.6037 - val_loss: 10.3892 - val_output_loss: 10.3623\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.0099 - output_loss: 2.9874 - val_loss: 2.5590 - val_output_loss: 2.5362\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.4281 - output_loss: 1.4063 - val_loss: 1.2931 - val_output_loss: 1.2717\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8226 - output_loss: 0.8014 - val_loss: 0.9886 - val_output_loss: 0.9675\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6687 - output_loss: 0.6479 - val_loss: 0.5397 - val_output_loss: 0.5188\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5803 - output_loss: 0.5595 - val_loss: 0.5622 - val_output_loss: 0.5415\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5468 - output_loss: 0.5262 - val_loss: 0.5300 - val_output_loss: 0.5094\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5365 - output_loss: 0.5159 - val_loss: 0.5226 - val_output_loss: 0.5020\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5300 - output_loss: 0.5094 - val_loss: 0.5434 - val_output_loss: 0.5228\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5207 - output_loss: 0.5002 - val_loss: 0.5473 - val_output_loss: 0.5268\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5149 - output_loss: 0.4945 - val_loss: 0.5415 - val_output_loss: 0.5211\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5112 - output_loss: 0.4909 - val_loss: 0.5222 - val_output_loss: 0.5018\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5071 - output_loss: 0.4868 - val_loss: 0.5143 - val_output_loss: 0.4939\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5037 - output_loss: 0.4833 - val_loss: 0.4899 - val_output_loss: 0.4694\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5089 - output_loss: 0.4884 - val_loss: 0.4947 - val_output_loss: 0.4743\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4999 - output_loss: 0.4795 - val_loss: 0.5277 - val_output_loss: 0.5073\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4972 - output_loss: 0.4768 - val_loss: 0.5135 - val_output_loss: 0.4932\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4950 - output_loss: 0.4746 - val_loss: 0.5009 - val_output_loss: 0.4805\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.5033 - output_loss: 0.4829 - val_loss: 0.5003 - val_output_loss: 0.4799\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5369 - output_loss: 0.5164 - val_loss: 0.5685 - val_output_loss: 0.5481\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5092 - output_loss: 0.4888 - val_loss: 0.5140 - val_output_loss: 0.4935\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4986 - output_loss: 0.4781 - val_loss: 0.5174 - val_output_loss: 0.4968\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4999 - output_loss: 0.4794 - val_loss: 0.4847 - val_output_loss: 0.4641\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4975 - output_loss: 0.4769 - val_loss: 0.5009 - val_output_loss: 0.4802\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5012 - output_loss: 0.4805 - val_loss: 0.5289 - val_output_loss: 0.5082\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4972 - output_loss: 0.4764 - val_loss: 0.5369 - val_output_loss: 0.5161\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5433 - output_loss: 0.5225 - val_loss: 0.5029 - val_output_loss: 0.4820\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6054 - output_loss: 0.5846 - val_loss: 0.5232 - val_output_loss: 0.5025\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6297 - output_loss: 0.6089 - val_loss: 0.4982 - val_output_loss: 0.4773\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5634 - output_loss: 0.5425 - val_loss: 0.6037 - val_output_loss: 0.5827\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7262 - output_loss: 0.7052 - val_loss: 1.1442 - val_output_loss: 1.1233\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2831 - output_loss: 1.2619 - val_loss: 1.7932 - val_output_loss: 1.7719\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.5614 - output_loss: 1.5403 - val_loss: 2.0170 - val_output_loss: 1.9962\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1496 - output_loss: 1.1291 - val_loss: 2.2209 - val_output_loss: 2.2002\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0933 - output_loss: 1.0729 - val_loss: 0.5385 - val_output_loss: 0.5185\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7866 - output_loss: 0.7666 - val_loss: 0.6195 - val_output_loss: 0.5993\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6446 - output_loss: 0.6245 - val_loss: 0.7105 - val_output_loss: 0.6907\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5828 - output_loss: 0.5628 - val_loss: 0.5488 - val_output_loss: 0.5290\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5440 - output_loss: 0.5241 - val_loss: 0.4338 - val_output_loss: 0.4140\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4730 - output_loss: 0.4533 - val_loss: 0.4764 - val_output_loss: 0.4566\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.5281 - output_loss: 0.5083 - val_loss: 0.6100 - val_output_loss: 0.5900\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.4819 - output_loss: 0.4621 - val_loss: 0.4549 - val_output_loss: 0.4351\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.4594 - output_loss: 0.4397 - val_loss: 0.4633 - val_output_loss: 0.4436\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4617 - output_loss: 0.4420 - val_loss: 0.4639 - val_output_loss: 0.4442\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4600 - output_loss: 0.4403 - val_loss: 0.4599 - val_output_loss: 0.4402\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4686 - output_loss: 0.4490 - val_loss: 0.4676 - val_output_loss: 0.4481\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4633 - output_loss: 0.4437 - val_loss: 0.4592 - val_output_loss: 0.4397\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4699 - output_loss: 0.4503 - val_loss: 0.4881 - val_output_loss: 0.4685\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4695 - output_loss: 0.4500 - val_loss: 0.4717 - val_output_loss: 0.4523\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4568 - output_loss: 0.4373 - val_loss: 0.4828 - val_output_loss: 0.4634\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4744 - output_loss: 0.4550\n",
      "Test Loss: [0.47437727451324463, 0.4550386369228363]\n"
     ]
    }
   ],
   "source": [
    "nam.compile(optimizer=Adam(learning_rate=0.01), loss={\"output\": \"mse\"})\n",
    "\n",
    "# Train the model\n",
    "nam.fit(nam.training_dataset, epochs=50, validation_data=nam.validation_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = nam.evaluate(nam.validation_dataset)\n",
    "print(\"Test Loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xDl_venv",
   "language": "python",
   "name": "xdl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
